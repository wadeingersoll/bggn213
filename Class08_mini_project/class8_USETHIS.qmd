---
title: "Class 8: Breast Cancer Analysis"
author: "Wade Ingersoll (PID: 69038080)"
format: pdf
toc: true
---

## Background

The goal of today's mini-project is to explore a complete analysis using the unsupervised learning techniques covered in the last class. We will extend what we learned by combining PCA as a preprocessing step to clustering using data that consist of measurements of cell nuclei of human breast massess.

The data itself comes from the Wisconsin Breast Cancer Diagnostic Data Set first reported by K. P. Benne and O. L. Mangasarian: “Robust Linear Programming Discrimination of Two Linearly Inseparable Sets”.

Values in this data set describe characteristics of the cell nuclei present in digitized images of a fine needle aspiration (FNA) of a breast mass.

## Data import

The data is available as a CSV from class website:

```{r}
wisc.df <- read.csv("WisconsinCancer (2).csv", row.names=1)
```

Make sure we do not include sample id or diagnosis columns in the data that we analyze below.

```{r}
diagnosis <- as.factor(wisc.df$diagnosis)
wisc.data <- wisc.df[, -1]
dim(wisc.data)
```

## Exploratory data analysis

> Q1. How many observations are in this dataset?

> Answer: There are `r nrow(wisc.data)` observations/samples/patients in the data set.
I used nrow(wisc.data) to find that number.

> Q2. How many of the observations have a malignant diagnosis?

> Answer: There are 212; see code below (two ways to determine)

```{r}
sum(wisc.df$diagnosis == "M")
```

```{r}
table(wisc.df$diagnosis)
```


> Q3. How many variables/features in the data are suffixed with _mean?

> Answer: There are 10; see code below

```{r}
#colnames(wisc.data)
length( grep("_mean", colnames(wisc.data)) )
```

Easier to read: 

```{r}
n <- colnames(wisc.data)
inds <- grep("_mean", n)
length(inds)
```


## Principal Component Analysis

The main function in Base R for PCA is called `prcomp()`. An optional argument `scale` should nearly always be switched to `scale=TRUE` for this function.

```{r}
wisc.pr <- prcomp(wisc.data, scale=TRUE)
summary(wisc.pr)
```

>Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

> The answer is 0.4427203, see code below

```{r}
pr.var <- wisc.pr$sdev^2
pr.var / sum(pr.var)
```


>Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

> Answer: Three PCs are required (PC1-3). See summary below.

```{r}
summary(wisc.pr)
```


>Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

> Answer: Seven PCs are required (PC1-7). See summary below.

```{r}
summary(wisc.pr)
```


## Interpreting PCA results

> Create a biplot of the wisc.pr using the `biplot()` function.

```{r}
biplot(wisc.pr)
```


>Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

> Answer: What stands out is that the plot is very cluttered with lots of overlap between labels, making it very difficult to interpret the PCA result.


## Scatter plot observations by components 1 and 2

```{r}
plot( wisc.pr$x[,1], wisc.pr$x[,2] , col = diagnosis , 
     xlab = "PC1", ylab = "PC2")
```
\newpage
>Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

> Answer: I notice the first plot (PC1 and 2) has less overlap between clusters (i.e. it has a cleaner cut). See plot for PC1 and 3 below.

```{r}
# Repeat for components 1 and 3
plot( wisc.pr$x[,1], wisc.pr$x[,3] , col = diagnosis , 
     xlab = "PC1", ylab = "PC3")
```

Let's make our main result figure - the "PC plot" or "score plot", "ordienation plot"...

```{r}
library(ggplot2)

ggplot(wisc.pr$x) +
  aes(PC1, PC2, col=diagnosis) +
  geom_point()
```

## Variance Explained

Calculate variance of each component

```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

Variance explained by each principal component: pve
```{r}
pve <- pr.var / sum(pr.var)
```

Plot variance explained for each principal component

```{r}
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

Alternative scree plot of the same data, note data driven y-axis

```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```
ggplot based graph

```{r}
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

## Communicating PCA Results

> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean? This tells us how much this original feature contributes to the first PC.

> Answer: Using the code below, the loading vector is -0.26085376

```{r}
wisc.pr$rotation[,1]
```


## Hierarchical Clustering

```{r}
apply(scale(wisc.data), 2, sd)
```

## Combining PCA and clustering

```{r}
d <- dist( wisc.pr$x[,1:3] )
wisc.pr.hclust <- hclust(d, method="ward.D2")
plot(wisc.pr.hclust)
```








```{r}
data.scaled <- scale(wisc.data)
data.dist <- dist(data.scaled)
wisc.hclust <- hclust(data.dist, method="complete")
plot(wisc.hclust)
abline(h=19, col='red', lty=2)
```
> Q10. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

> Answer: According to the plot above, **19** is the height at which the clustering model has 4 clusters.


## Selecting Number of Clusters

Get my cluster membership vector
```{r}
grps <- cutree(wisc.pr.hclust, h=70)
table(grps)
```

```{r}
table(diagnosis)
```

## Senstivity

Make a "cross-table"

```{r}
table(grps, diagnosis)
```
TP: 179


FP: 24

Sensitivity: TP/(TP+FN) = `r 179/(179+24)`


## Using Different Methods ("single")

> Q12. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

> Answer: My favorite method is "ward.D2" because it generates a plot that allows for use of abline() to find where the clustering model has 4 clusters.

```{r}
d <- dist( wisc.pr$x[,1:3] )
wisc.pr.hclust <- hclust(d, method="single")
plot(wisc.pr.hclust)
```

## Combining Methods

Clustering on PCA results

```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method = "ward.D2")
plot(wisc.pr.hclust)
```

Determine if these two clusters are malignant and benign

```{r}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
```

```{r}
table(grps, diagnosis)
```
```{r}
plot(wisc.pr$x[,1:2], col=grps)
```
```{r}
plot(wisc.pr$x[,1:2], col=diagnosis)
```
```{r}
g <- as.factor(grps)
levels(g)
```

```{r}
g <- relevel(g,2)
levels(g)
```

```{r}
# Plot using our re-ordered factor 
plot(wisc.pr$x[,1:2], col=g)
```

## Use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7]
```{r}
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method="ward.D2")
```

Cut this hierarchical clustering model into 2 clusters and assign the results to wisc.pr.hclust.clusters.

```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=4)
```

Using table(), compare the results from your new hierarchical clustering model with the actual diagnoses.

```{r}
# Compare to actual diagnoses
table(wisc.pr.hclust.clusters, diagnosis)
```
> Q13. How well does the newly created model with four clusters separate out the two diagnoses?

> Answer: It does so much better.


>Q14. How well do the hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.

> Answer: They are much better.

